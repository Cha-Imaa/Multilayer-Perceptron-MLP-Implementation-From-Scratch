# Multilayer-Perceptron-MLP-Implementation-From-Scratch
# Hello
# MLP From Scratch

## Overview

In this repository, we implement a Multilayer Perceptron (MLP) entirely from scratch, including the construction of linear layers, activation functions, and batch normalization. Additionally, we delve into optimization algorithms essential for training our MLP. The primary aim of this repo is to foster a deep understanding of the internal mechanics of neural networks and to provide hands-on experience with the fundamental algorithms that power deep learning, without relying on high-level frameworks. 

## Key Features

- **Linear Layer Implementation**: Craft custom linear layers as the building blocks of the MLP.
- **Activation Functions**: Implement various activation functions, including Sigmoid and ReLU, to introduce non-linearity.
- **Batch Normalization**: Integrate batch normalization to enhance training stability.
- **MLP Construction**: Combine the above components to create versatile MLP architectures.

## Training Mechanics

- Perform **forward inference** to make predictions.
- Implement **Mean Squared Error and Cross-Entropy Loss** functions for output evaluation.
- Calculate **derivatives** for backpropagation.
- Use **Stochastic Gradient Descent (SGD)** for optimization.
